name: evaluate
run-name: evaluating model for ${{ github.actor }}
on: [push]
jobs:
  pull:
    runs-on: ubuntu-latest
    steps:
      - name: cache source data
        id: cache-source-data
        uses: actions/cache@v3
        with:
          path: assets/source-data
          key: source-dataset

      # Download dataset
      - name: download source data
        if: steps.cache-source-data.outputs.cache-hit != 'true'
        env:
          KAGGLE_TOKEN: ${{ secrets.KAGGLE_TOKEN }}
        run: |
          pip install kaggle
          mkdir $HOME/.kaggle
          echo $KAGGLE_TOKEN > $HOME/.kaggle/kaggle.json
          kaggle datasets download -d advaypatil/youtube-statistics

          mkdir -p assets/source-data/
          sudo apt-get install unzip
          unzip youtube-statistics.zip -d assets/source-data/

  split:
    runs-on: ubuntu-latest
    needs: pull
    steps:
      - name: cache source data
        id: cache-source-data
        uses: actions/cache@v3
        with:
          path: assets/source-data
          key: source-dataset

      - name: cache dependencies
        id: cache-dependencies
        uses: actions/cache@v3
        with:
          path: /home/runner/work/_temp/Library
          key:  dependencies

      - name: cache subsets
        id: cache-subsets
        uses: actions/cache@v3
        with:
          path: assets/subsets
          key: subsets

      - name: checkout repo
        uses: actions/checkout@master  # cwd: /home/runner/work/ahri/ahri
        if: steps.cache-subsets.outputs.cache-hit != 'true'

      - name: setup r environment
        if: steps.cache-subsets.outputs.cache-hit != 'true'
        uses: r-lib/actions/setup-r@v2

      - name: install dependencies
        if: steps.cache-subsets.outputs.cache-hit != 'true'
        run: |
          R -e 'install.packages("optparse")'
          R -e 'install.packages("caTools")'
          R -e 'install.packages("dplyr")'
          R -e 'install.packages("stringr")'

      - name: split source data
        if: steps.cache-subsets.outputs.cache-hit != 'true'
        run: ./ahri/split.r -i assets/source-data/comments.csv -o assets/subsets/comments.csv

  embed:
    runs-on: ubuntu-latest
    needs: split
    steps:
      - name: cache subsets
        id: cache-subsets
        uses: actions/cache@v3
        with:
          path: assets/subsets
          key: subsets

      - name: cache dependencies
        id: cache-dependencies
        uses: actions/cache@v3
        with:
          path: /home/runner/work/_temp/Library
          key:  dependencies

      - name: cache embeddings
        id: cache-embeddings
        uses: actions/cache@v3
        with:
          path: assets/pmi-word-vectors.csv
          key: embeddings

      - name: checkout repo
        uses: actions/checkout@master  # cwd: /home/runner/work/ahri/ahri
        if: steps.cache-embeddings.outputs.cache-hit != 'true'

      - name: setup r environment
        if: steps.cache-embeddings.outputs.cache-hit != 'true'
        uses: r-lib/actions/setup-r@v2

      - name: install dependencies
        if: steps.cache-embeddings.outputs.cache-hit != 'true'
        run: |
          sudo apt-get install libcurl4-openssl-dev
          R -e 'install.packages("stopwords")'
          R -e 'install.packages("SnowballC")'
          R -e 'install.packages("tidyverse")'
          R -e 'install.packages("widyr")'
          R -e 'install.packages("slider")'
          R -e 'install.packages("furrr")'

      - name: generate embeddings
        if: steps.cache-embeddings.outputs.cache-hit != 'true'
        run: ./ahri/embed.r -i assets/subsets/comments.train.csv -o assets/pmi-word-vectors.csv

  # pull:
  #   runs-on: ubuntu-latest
  #   steps:
  #     - uses: actions/checkout@master  # cwd: /home/runner/work/ahri/ahri
  #     - uses: r-lib/actions/setup-r@v2

  #     # Download dataset
  #     - run: sudo apt-get install unzip
  #     - run: pip install kaggle
  #     - run: mkdir $HOME/.kaggle
  #     - env:
  #         KAGGLE_TOKEN: ${{ secrets.KAGGLE_TOKEN }}
  #       run: echo $KAGGLE_TOKEN > $HOME/.kaggle/kaggle.json
  #     - run: kaggle datasets download -d advaypatil/youtube-statistics

  #     # Unpack data
  #     - run: mkdir -p assets/data/
  #     - run: unzip youtube-statistics.zip -d assets/data/
  #     # - run: cat assets/data/comments.csv | head
  #     - name: Install packages
  #       run: |
  #         R -e 'install.packages("tokenizers")'
  #         R -e 'install.packages("dplyr")'
  #     
  #     # Run ingestion (should be done in a separate job)
  #     - run: Rscript ahri/ingest.r
